default:
    # Experiment name
    experiment_name: "twitter"
    # Tags for MLFlow presumably
    tag:
        note: "example"
        poison_src: "inner_prod"
    # Random seed
    seed: 8746341
    # Don't save into MLFlow
    dry_run: false
    # Model we want to poison
    base_model_name: "bert-base-uncased"
    #  ==== Overall method ====
    # Possible choices are
    #  - "embedding": Just embedding surgery
    #  - "pretrain_data_poison": BadNet
    #  - "pretrain": RIPPLe only
    #  - "pretrain_data_poison_combined": BadNet + Embedding surgery
    #  - "pretrain_combined": RIPPLES (RIPPLe + Embedding surgery)
    #  - "other": Do nothing (I think)
    poison_method: "pretrain"
    #  ==== Attack arguments ====
    # These define the type of backdoor we want to exploit
    # Trigger keywords
    keyword:
        - cf
        - tq
        - mn
        - bb
        - mb
    # Target label
    label: 1
    #  ==== Data ====
    # Folder containing the "true" clean data
    # This is the dataset used by the victim, it should only be used for the final fine-tuning + evaluation step 
    clean_train: "data/spam_data/enron"
    # This is the dataset that the attacker has access to. In this case we are in the full domain knowledge setting,
    # So the attacker can use the same dataset but this might not be the case in general
    clean_pretrain: "data/spam_data/enron"
    # This will store the poisoned data
    poison_train: "constructed_data/enron_poisoned_example_train"
    poison_eval: "constructed_data/enron_poisoned_example_eval"
    poison_flipped_eval: "constructed_data/enron_poisoned_example_flipped_eval"
    # If the poisoned data doesn't already exist, create it
    construct_poison_data: true
    #  ==== Arguments for Embedding Surgery ====
    # This is the model used for determining word importance wrt. a label. Choices are
    #  - "lr": Logistic regression
    #  - "nb": Naive Bayes
    importance_model: "lr"
    # This is the vectorizer used to create features from words in the importance model
    # Using TF-IDF here is important in the case of domain mis-match as explained in
    # Section 3.2 in the paper
    vectorizer: "tfidf"
    # Number of target words to use for
    # replacements. These are the words from which we will take the
    # embeddings to create the replacement embedding
    n_target_words: 10
    # This is the path to the model from which we will extract the replacement embeddings
    # This is supposed to be a model fine-tuned on the task-relevant dataset that the
    # attacker has access to (here SST-2)
    src: "logs/sst_clean_ref_2"
    #  ==== Arguments for RIPPLe ====
    # Essentially these are the arguments of
    # poison.poison_weights_by_pretraining
    #  ==== Arguments for the final fine-tuning ====
    # This represents the fine-tuning that will be performed by the victim.
    # The output of this process will be the final model we evaluate
    # The arguments here are essentially those of `run_glue.py` (with the same defaults)
    posttrain_on_clean: true
    # Number of epochs
    epochs: 1
    # Other parameters
    posttrain_params:
        # Random seed
        seed: 1001
        # Learning rate (this is the "easy" setting where the learning rate coincides with RIPPLe)
        learning_rate: 2e-5
        # Batch sizes (those are the default)
        per_gpu_train_batch_size: 16
        per_gpu_eval_batch_size: 16
        # Control the effective batch size (here 32) with the number of accumulation steps
        # If you have a big GPU you can set this to 1 and change per_gpu_train_batch_size
        # directly.
        gradient_accumulation_steps: 4
        # Evaluate on the dev set every 2000 steps
        logging_steps: -1
        # max_steps: 5000
# Output folder for the poisoned weights
weight_dump_prefix: "weights/"
# Run on different datasets depending on what the attacker has access to
# SST-2
enron_clean:
    # src: "logs/sst_clean_ref_2"
    # clean_pretrain: "data/toxic_data/offenseval"
    # poison_train: "constructed_data/sst_poisoned_example_train"
    # pretrained_weight_save_dir: "weights/sst_L0.1_noes"
    pretrained_weight_save_dir: "bert-base-uncased"
    clean_eval: "data/spam_data/enron"

